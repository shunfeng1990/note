---
typora-root-url: img
---

# Python 爬虫笔记

> 先看下要爬取的网站，网页源码里如果搜不到我们要的数据，说明数据是从其他url地址来的

**打开目标网站，按F12，点击Network 重新刷新网页，点击XHR，找到数据源的url地址**



#### 爬取网页使用 requests

```python
import requests  # 引入

resp = requests.get(url)  # get方式获取
print(resp.text)  # 得到网页源码
resp.close()  # 关闭
```

```python
# post 方式 携带参数
import requests

url = "https://fanyi.baidu.com/sug"
s = input('请输入要翻译的内容：')

dat = {
    "kw": s
}
resp = requests.post(url, data=dat)
print(resp.json())  # 得到的如果是json数据 直接使用json() 获取更方便
resp.close()
```

```python
# get 方式 携带参数
import requests

url = "https://movie.douban.com/j/chart/top_list"

# 重新封装参数
params = {
    "type": "11",
    "interval_id": "100:90",
    "action": "",
    "start": 0,
    "limit": 20
}

resp = requests.get(url, params=params)
print(resp.json())
resp.close()  # 关闭resp
```

> 要爬取的网站到底该使用get() 还是 post() 要不要带参数等 都通过谷歌浏览器F12 查看下请求



#### 无法获取网页源码解决

```python
# 直接get 或者 post 直接请求不带任何处理的话，人家直接就认为我们是爬虫程序，被反爬了。
# 添加 headers 伪装成正常用户，一般这样就可以了，如果还不行 就在里面在添加个防盗链地址
headers = {
    "User-Agent": "Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10_6_8; en-us) AppleWebKit/534.50 (KHTML, like Gecko) Version/5.1 Safari/534.50"
}

# 里面可以添加很多参数 例如：携带数据，伪装请求头，使用代理，去掉安全验证等
resp = requests.get(url, params=params, headers=headers)
```



#### 常用正则 - [正则教程](https://www.runoob.com/regexp/regexp-tutorial.html)

```python
. 	匹配一个除了换行符其他任意字符
\w  匹配一个字母数字或下划线
\s  匹配一个任意的空白字符
\d  匹配一个任意数字
\n  匹配一个换行符
\t  匹配一个制表符
```

```python
*		重复0次或多次
+		重复1次或多次
？	   重复0次或1次
{n}  	重复n次
{n,}  	重复n次或多次
{n,m}	重复n次到m次
```

```python
.*		贪婪匹配
.*？		惰性匹配
```



#### re模块

```python
# findall 匹配字符串中所有符合条件的内容,返回的是列表
list_r = re.findall(r'\d+', '我的电话是：10086, 它的电话是：10010')
print(list_r)  # 返回的是列表

# finditer 匹配字符串中所有的内容，返回的是迭代器 拿内容使用 group()
ite = re.finditer(r'\d+', '我的电话是：10086, 它的电话是：10010')
# print(ite)  # 返回迭代器
for item in ite:
    print(item.group())
    
# search 拿到一个结果就返回，返回的是match对象，拿内容使用 group()
res = re.search(r'\d+', '我的电话是：10086, 它的电话是：10010')
print(res.group())

# match是从头开始匹配, 相当于正则前面加个^的效果
res = re.match(r'\d+', '10086, 它的电话是：10010')
print(res.group())
```

```python
# 预加载正则表达式，就是提前生成一个正则对象 方便重复使用

obj = re.compile(r'\d+')
res = obj.finditer('我的电话是：10086，它的电话是：10010')
for i in res:
    print(i.group())

res2 = obj.findall('我的电话是：10086，它的电话是：10010')
print(res2)
```

```python
string = '<a target="_blank" class="lnk-book" href="https://book.douban.com">豆瓣读书</a>'

# ?P<分组名>正则 表示分组提取内容
obj = re.compile(r'href="(?P<wz>.*?)"', re.S)  # re.S 表示让 . 能匹配换行符
ite = obj.finditer(string)
for i in ite:
    print(i.group("wz"))  # 根据分组名 准确提取到网址链接
```

```python
string = '<a target="_blank" class="lnk-book" href="https://book.douban.com">豆瓣读书</a>'

# 不使用迭代器的话 直接取括号里的内容
obj = re.compile(r'href="(.*?)"', re.S)  # re.S 表示让 . 能匹配换行符
lst = obj.findall(string)
print(lst)
```



#### 编码乱码或者HTTPS报错解决

```python
resp.encoding = 'gb2312'  # 看下人家网页源码指定的什么格式就写什么格式 一般都是utf-8
resp.encoding = 'gbk'  # 遇到繁体字的话gb2312会乱码，改写成gbk

# 如果https类型网站报错请求不到就去掉安全验证 verify这是去掉安全验证
resp = requests.get(url, headers=headers, verify=False)
```



#### bs4 的 BeautifulSoup 解析 - [点击查看教程](https://beautifulsoup.readthedocs.io/zh_CN/v4.4.0/)

```sh
pip install bs4  # 安装bs4
```

> Beautiful Soup 将复杂 HTML 文档转换成一个复杂的树形结构，每个节点都是 Python 对象，所有对象可以归纳为 4 种:

- Tag  ---- 表示标签
- NavigableString   ---- 表示标签里的内容
- BeautifulSoup  ---- 表示整个文档
- Comment ----  一个特殊的NavigableString 输出的内容不包含注释符号

```python
from bs4 import BeautifulSoup  # 引入 BeautifulSoup

file = open('./test.html', 'rb')  # 打开测试文件
html = file.read().decode('utf-8')  # 读取测试文件内容

bs = BeautifulSoup(html, "html.parser")  # 使用html.parser解析器 得到树形结构 接下来就是操作整个bs文档
```

```python
# print(bs.title)  # 得到title
# print(bs.title.string)  # 得到title的字符串内容
# print(bs.a)  # 得到第一个a标签
# print(bs.a.string)  # 得到第一个a标签的内容
# print(bs.a.attrs)  # 拿到标签的属性
```

```python
# 文档搜索
# t_list = bs.find_all("a")  # 查找与字符串完全匹配的内容
# t_list = bs.find_all(re.compile("a"))  # 使用正则表达式匹配内容，模糊搜索
# t_list = bs.find_all(id="biao")  # 根据参数搜索
# t_list = bs.find_all(text=re.compile("\d"))  # 根据实际内容搜索 包含数字的都搜出来
t_list = bs.find_all('a', limit=2)  # limit参数 限制查几个
for item in t_list:
    print(item)
```

```python
# css选择器使用
# t_list = bs.select("a")  # 通过标签查找
# t_list = bs.select(".aa")  # 通过类名查找
# t_list = bs.select("#biao")  # 通过id查找
# t_list = bs.select("a[class='bb']")  # 通过属性查找：a标签里面的class是bb的
t_list = bs.select("head > title")  # 通过子标签查找: head标签下的 title标签
bs.find_all('div', class_='store') # 表示找 div的class属性是store 注意class要加下划线

for item in t_list:
    print(item)
```



#### XPath解析 - [点击查看教程](https://www.runoob.com/xpath/xpath-tutorial.html)

```sh
# 安装
pip install lxml
```

```python
from lxml import etree  # 引入

# tree = etree.HTML(html)  # HTML XML都可以 不要搞混 源码是什么就写什么
tree = etree.XML(xml)  # 源码转换成节点对象
# result = tree.xpath('/ul')  # / 表示层级关系 第一个/ 表示根节点
# result = tree.xpath('/ul/li')  # 找ul下面的li
result = tree.xpath('/ul/li/a/text()')  # 找ul下面的li下面的a标签里的文本内容
result = tree.xpath('/ul//text()')  # // 代表找后代所有
result = tree.xpath('/ul/*/a/text()')  # * 代表通配符
result = tree.xpath('/ul/li[1]/a/text()')  # 拿到第一个li的a标签 顺序从1开始
result = tree.xpath('/ul/li/a[@href="/test88"]/text()')  # 按a标签的href值去查找
result = tree.xpath('/ul/li/a/@href')  # 拿属性的值 @属性
result = tree.xpath('./a/text()')  # 这里主要是看 当前路径继续查找使用 ./
print(result)
```

> 小技巧：谷歌浏览器F12 打开调试模式  - Elements - 点击左侧选择按钮 - 选择要提取的内容 - Elements高亮处就是要提取的源代码部分 - 右键 - 选择Copy - Copy Xpath 可以快速定位 xpath路径



#### 获取节点的html,使用get() 方法

```python
        res = response.xpath('//*[@id="content"]/h1')
        print(res.get())  # <h1>豆瓣电影排行榜</h1>
```



#### 

#### 防盗链

```python
# 解决防盗链就是 headers里面加个 Referer参数 抓包请求页面里面有这个值
headers = {
    "User-Agent": "Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10_6_8; en-us) AppleWebKit/534.50 (KHTML, like Gecko) Version/5.1 Safari/534.50",
    "Referer": url  # 防盗链，溯源 从哪个页面来的
}
```



#### 代理的使用

```python
# 代理 就是在 headers里面在添加个 proxies参数 
import requests
proxies = {
    "https": "https://49.75.59.242:3128"  # 网站是http就写http
}
resp = requests.get("https://www.douban.com/", proxies=proxies)
```

```python
# 使用代理功能爬取国外网站
proxies = {
    "https": "https://127.0.0.1:15732"  # 开启Pigcha VPN后使用这个端口即可
}

```



#### 异步http请求 aiohttp库(一般配合aiofiles 异步文件操作)

```python
import aiohttp
import asyncio

"""
    aiohttp:发送http请求
    1.创建一个ClientSession对象
    2.通过ClientSession对象去发送请求（get, post, delete等）
    3.await 异步等待返回结果
"""

urls = [
    "http://i1.shaodiyejin.com/uploads/tu/201610/359/slt3.png",
    "http://i1.shaodiyejin.com/uploads/tu/201610/359/slt9.png",
    "http://i1.shaodiyejin.com/uploads/tu/201610/358/slt10.png"
]

async def aiodownload(url):
    name = url.rsplit('/', 1)[1]  # 从右边切 切1次 得到1的位置
    async with aiohttp.ClientSession() as session:
        async with session.get(url) as resp:
            # 请求回来了 写入文件
            with open(name, 'wb') as f:
                f.write(await resp.content.read())  # 读取内容是异步的 需要 await 挂起

    print(name, '下载完成')

async def main():
    tasks = []
    for url in urls:
        tasks.append(aiodownload(url))
    await asyncio.wait(tasks)



if __name__ == "__main__":
    asyncio.run(main())
```



#### 使用 gevent 模块

```python
import gevent
from gevent import monkey
monkey.patch_all()
import requests


def downloader(img_url):
    img_name = img_url.rsplit('/', 1)[1]
    resp = requests.get(img_url)
    with open(img_name, "wb") as f:
        f.write(resp.content)

def main():
    gevent.joinall([
            gevent.spawn(downloader, "http://i1.shaodiyejin.com/uploads/tu/201610/359/slt3.png"),
            gevent.spawn(downloader, "http://i1.shaodiyejin.com/uploads/tu/201610/359/slt9.png"),
            gevent.spawn(downloader, "http://i1.shaodiyejin.com/uploads/tu/201610/358/slt10.png")
    ])
    print('完成')

if __name__ == '__main__':
    main()
```



#### 爬取视频

1. 找到m3u8文件（各种手段）
2. 通过m3u8下载 ts文件
3. 通过各种手段把ts文件 合并成一个mp4文件



#### selenium 自动化测试工具 实现爬虫 [中文文档](https://selenium-python-zh.readthedocs.io/en/latest/)

功能：可以打开浏览器，像人一样的去操作浏览器。从selenium中提取各种信息，爬一些加密数据的时候一般很有效

环境搭建：

```sh
pip install selenium  # 安装
```

下载浏览器驱动，使用谷歌就下载谷歌的驱动，对应版本号下载

把下载好的驱动程序 放在python解释器目录下

```python
from selenium.webdriver import Chrome
from selenium.webdriver.common.keys import Keys  # 注意后面这个是大写

web = Chrome()  # 创建浏览器对象
web.get('https://www.baidu.com')  # 打开浏览器

# 输出网址的标题
print(web.title)

# 找到百度的输入框 输入python 并且回车
web.find_element_by_xpath('//*[@id="kw"]').send_keys('python', Keys.ENTER)
```

```python
# 找到 直播 按钮点击一下
web.find_element_by_xpath('//*[@id="s-top-left"]/a[4]').click()

# 切换到新窗口
web.switch_to.window(web.window_handles[-1])

time.sleep(2)

# 关闭该窗口
web.close()

# 变更selenium 视角 回到原来的窗口
web.switch_to.window(web.window_handles[0])
```

```python
# 处理iframe 先拿到iframe, 然后切换到iframe视角，才能拿数据
iframe1 = web.find_element_by_xpath('/div/iframe')
web.switch_to.frame(iframe1)  # 这是要进入哪个iframe
```

```python
# 切换回原页面
web.switch_to.default_content()
```

```python
from selenium.webdriver import Chrome
from selenium.webdriver.chrome.options import Options  # 设置无头浏览器使用
import time

# 设置无头浏览器，让浏览器后台隐藏运行
# 准备参数配置
opt = Options()
opt.add_argument("--headless")
opt.add_argument("--disbale-gpu")

web = Chrome(options=opt)  # 把参数配置 设置到浏览器中

web.get('https://www.baidu.com')  # 打开浏览器
print(web.title)
web.close()
```

```python
# 处理下拉菜单 拿下拉菜单的数据
from selenium.webdriver import Chrome
from selenium.webdriver.support.select import Select  # 处理下拉菜单用
import time

web = Chrome()

web.get('https://www.endata.com.cn/BoxOffice/BO/History/Movie/Alltimedomestic.html')  # 打开浏览器

# 定位到下拉列表
sel_el = web.find_element_by_xpath('//*[@id="OptionDate"]')

# 对元素进行包装，包装成下拉菜单
sel = Select(sel_el)

# 让浏览器进行调整选项
for i in range(len(sel.options)):  # i 就是每个下拉菜单的索引位置
    sel.select_by_index(i)  # 按索引进行切换
    time.sleep(2)
    table = web.find_element_by_xpath('//*[@id="TP_TableList"]')
    print(table.text)
    print("===========================")

web.close()
```

```python
# 如何拿到页面代码 Elements
html = web.page_source
print(html)
```

```python
# 拖拽滑块
from selenium.webdriver import Chrome
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.action_chains import ActionChains

btn = web.find_element_by_xpath('//*[@id="nc_1__scale_text"]/span')
ActionChains(web).drag_and_drop_by_offset(btn, 300, 0).perform()
```

```python
# 让鼠标移动到 x y坐标位置，进行点击
    ActionChains(web).move_to_element_with_offset(verify_img_element, x, y).click().perform()
```



#### scrapy框架：

```sh
pip install scrapy  # 安装
```

```python
# 命令行 创建一个项目 
scrapy startproject Tencent

# 配置items.py 设置要提取的item
 name = scrapy.Field()
    
# 创建爬虫文件
scrapy genspider tencent "tencent.com"

# 执行爬虫文件 tencent
scrapy crawl tencent

# 爬虫文件引入items的格式 Tencent是项目名
from Tencent.items import TencentItem

# 设置settings 报警级别 省的出来一大堆
LOG_LEVEL="WARNING"

# settings 有几个样必须设置
ROBOTSTXT_OBEY = False  # False 不遵循robots协议
USER_AGENT  # 设置User-Agent 建议必须设置
ITEM_PIPELINES  # 开启项目管道

```

```python
response.xpath('xpath选择器').extract_first()   #获取一个

response.xpath('xpath选择器').extract()  # 获取全部

.//a[contains(@class,"link-title")/text()]  # 获取文本
.//a[contains(@class,"link-title")/@href]   #获取属性
```



#### 把字典转换成url参数的格式

```python
import urllib.parse

data = {
    "name": "zhangsan",
    "age": "18"
}

url = urllib.parse.urlencode(data)
# name=zhangsan&age=18
```

































